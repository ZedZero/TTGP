\subsection{Gaussian Processes}
  Here we provide a brief review of standard Gaussian process methods and their
  limitations.

  A Gaussian process is a collection of random variables, any finite number of
  which have a joint normal distribution. A GP $f$ taking place in $\R^D$ is
  fully defined by its mean $m: \R^D \rightarrow \R$ and covariance
  $k: \R^D \times \R^D \rightarrow \R$ functions. For every $t_1, t_2, \ldots, t_n \in \R^D$
  \[
    f(t_1), f(t_2), \ldots, f(t_n) \sim \N(m, K),
  \]
  where $m = (m(t_1), m(t_2), \ldots, m(t_n))^T \in \R^n$, and
  $K\in \R^{n \times n}$ is the matrix comprised of pairwise values of covariance
  function $k$. Below we will use notation $K(A, B)$ for the matrix of pairwise
  values of covariance function $k$ on points from sets $A$ and $B$.

  Consider regression task. The dataset  consists of
  $n$ objects $X = (x_1, \ldots, x_n)^T \in \R^{n \times D}$, and target values
  $y = (y_1, y_2, \ldots, y_n)^T \in \R^n$. We will assume that the data is
  generated by a latent Gaussian process $f$ taking place in the feature space
  and denote the value of the process at data point $x_i$ by $f_i$ for all
  $i = 1, 2, \ldots, n$. We will use a zero-mean GP prior with covariance function
  $k$ for $f$. We will assume that the observed target variables $y_i$
  are a noisy version of $f_i$:
  \[
    p(y_i | f_i) = \N(y_i | f_i, \nu^2 I),
  \]
  where $\nu^2$ is the noise variance.

  Assume that we want to predict the values of the process $f_*$ at a set of test
  points
  $X_*$. As the joint distribution of $y$ and $f_*$ is Gaussian, we can anlytically
  compute the conditional distribution
  \[
    p(f_* | y, X, X_*) = \N(f_* | \hat m, \hat K),
  \]
  where
  \[
      \hat m = K(X_*, X) (K(X, X) + \nu^2 I)^{-1} y,
  \]
  \[
      \hat K = K(X_*, X_*) - K(X_*, X)(K(X, X) + \nu^2 I)^{-1} K(X, X_*)).
  \]

  Popular covariance functions usually have a set of hyper-parameters $\theta$.
  For example, the RBF kernel
  \[
    k_{\mbox{\scriptsize RBF}} (x, x') = \sigma_f^2 \exp\left(- 0.5 \Vert x - x'\Vert^2 / l^2 \right)
  \]
  has two parameters $l$ and $\sigma_f$. In order to fit the model to the data,
  we can maximize the marginal likelihood of the process with respect to these
  parameters. In case of GP regression we can compute the marginal likelihood
  analytically
  \[
    \log p(y) = -\frac 1 2 y^T (K_{\theta}(X, X) + \nu^2 I)^{-1} y -
    \frac 1 2 \log |K_{\theta} (X, X) + \nu^2 I| - \frac n 2 \log 2 \pi,
  \]
  where $K_{\theta}(X, X)$ denotes the covariance matrix for the specific
  value of the hyper-parameters $\theta$.

  Note that the complexity of computing both predictive distribution and
  marginal likelihood is $\bigO(n^3)$.

  For classification we substitute the normal $p(y_i | f_i)$ distribution
  with a sigmoid
  \[
    p(y_i | f_i) = \sigma(y_i f_i).
  \]

  For classification both predictive distribution and marginal likelihood
  are intractable. For detailed description of GP regression and
  classification see \citet{rasmussen2006}

\subsection{Inducing Inputs}
\label{inducing_inputs}

  A number of approximate methods were developed to scale up Gaussian processes.
  \citet{hensman2013} proposed a variational lower bound that factorizes over
  observations for Gaussian process marginal likelihood. We rederive
  this bound here.

  Consider a set $Z \in \R^{m \times D}$ of $m$ points in the feature space.
  We will call points $Z$ inducing inputs or inducing points.
  We will introduce latent variables $u \in \R^m$ representing the values of
  the Gaussian process at these points. Consider the augmented model
  \[
    p(y, f, u) = p(y | f) p(f | u) p(u) = \prod_{i = 1}^ n p(y_i | f_i) p(f | u) p(u),
  \]
  where
  \begin{equation}
  \label{priors}
    p(f | u) = \N(f | \Knm \Kmm^{-1} u, \Knn - \Knm \Kmm^{-1} \Kmn),~~~p(u)=
    \N(u | 0, \Kmm).
  \end{equation}
  Here and below we ommit the inputs $X$ to simplify the notation.

  The standard variational lower bound is given by
  \begin{equation}
  \label{standard_elbo}
    \log p(y) \ge \E_{q(u, f)} \log \frac {p(y, f, u)} {q(u, f)} =
    \E_{q(f)} \log \prod_{i=1}^n p(y_i | f_i) - \KL(q(u, f) || p(u, f)),
  \end{equation}
  where $q(u, f)$ is the variational distribution over latent variables.
  Consider the following family of variational distributions
  \begin{equation}
  \label{var_family}
    q(u, f) = p(f | u) \N(u | \mu, \Sigma),
  \end{equation}
  where $\mu \in \R^m$ and $\Sigma \in \R^{m \times m}$ are variational
  parameters.

  Throughout the paper we will use the following notation.
  \[
    \Knn = K(X, X),~~~\Knm = K(X, Z),~~~\Kmn = K(Z, X) = \Knm^T,~~~\Kmm = K(Z, Z).
  \]

  We can then rewrite the KL-divergence tetm in (\ref{standard_elbo}) as follows
  \[
    \KL(q(u, f) || p(u, f)) = \KL(p(f | u) q(u) || p(f | u) p(u)) = \KL(q(u) || p(u)) =
  \]
  \[
    = \frac 1 2 \left(\log \frac{|\Kmm|}{|\Sigma|} - m + \tr(\Kmm^{-1} \Sigma + \mu^T \Kmm^{-1} \mu) \right).
  \]

  The marginal distribution over $f$ can be computed analytically
  \[
    q(f) = \N\left(f | \Knm \Kmm^{-1} \mu, \Knn + \Knm \Kmm^{-1} (\Sigma - \Kmm) \Kmm^{-1} \Kmn\right).
  \]
  We can then rewrite (\ref{standard_elbo}) as

  \begin{equation}
  \label{main_elbo}
    \log p(y) \ge \sum_{i=1}^n \E_{q(f_i)} \log p(y_i | f_i) - \KL(q(u) || p(u)).
  \end{equation}

  Note that the lower bound (\ref{main_elbo}) factorizes over observations and
  thus stochastic optimization can be applied to maximize this bound with respect
  to both kernel hyper-parameters $\theta$ and variational parameters $\mu$ and
  $\Sigma$. In case of regression we can rewrite (\ref{main_elbo}) in the closed
  form
  \begin{equation}
    \label{elbo_reg}
    \begin{split}
      \log p(y) \ge
      \sum_{i=1}^n \left ( \log \N (y_i | \ki^T \Kmm^{-1} \mu, \nu^2) -
        \frac 1 {2\nu^2} \tilde K_{ii} -
        \frac 1 {2\nu^2} \tr (\ki^T \Kmm^{-1} \Sigma \Kmm^{-1} \ki)
      \right )-
      \\
      - \frac 1 2 \left(
        \log \frac {|\Kmm|} {|\Sigma|} - m + \tr(\Kmm^{-1} \Sigma) +
        \mu^T \Kmm^{-1} \mu
      \right),
    \end{split}
  \end{equation}
  where $\ki \in \R^m$ is the $i$-th column of $\Kmn$ matrix and
  \[
    \tilde{K} = \Knn - \Knm \Kmm^{-1} \Kmn.
  \]
  At prediction time we can use the variational distribution as a substitute for
  posterior
  \[
    p(f_* | y) = \int p(f_*| f, u) p(f, u |y) d f d u \approx
    \int p(f_* | f, u) q(f, u) d u d f = \int p(f_* | u) q(u) du.
  \]

  The complexity of computing the bound (\ref{elbo_reg}) is $\bigO(n m^2 + m^3)$.
  \citet{hensman2015} proposes to use Gauss-Hermite quadratures to approximate the
  expectation term in (\ref{main_elbo}) for binary classification problem to
  obtain the same computational complexity $\bigO(nm^2 + m^3)$. This complexity
  allows to use Gaussian processes in tasks with millions of training samples,
  but these methods are limited to use small numbers of inducing points $m$,
  which hurts the predictive performance and doesn't allow to learn expressive
  kernel functions.

\subsection{KISS-GP}
\label{kiss_gp}

  \citet{saatci2012} noted that the covariance matrices computed at points on a
  multidimensional grid in the feature space can be represented as a Kronecker
  product if the kernel function factorizes over dimensions
  \begin{equation}
  \label{prod_kernel}
    k(x, x') = k_1(x^1, x'^1)\cdot k_2(x^2, x'^2)\cdot \ldots\cdot k_D(x^D, x'^D).
  \end{equation}
  Note, that many popular covariance functions, including RBF, belong to this class.
  Kronecker structure of covariance matrices allows to perform efficient inference
  for full Gaussian processes with inputs $X$ on a grid.

  \citet{wilson2015} proposed to set inducing inputs $Z$ on a grid:
  \[
    Z = Z^1 \times Z^2 \times \ldots \times Z^D,~~~~~Z^i \in \R^{m_i}~~~\forall i = 1, 2, \ldots, D.
  \]
  The number $m$  of inducing points is then given by
  \[
    m = \prod_{i=1}^D m_{i}.
  \]
  Let the covariance function satisfy (\ref{prod_kernel}). Then the covariance
  matrix $\Kmm$ can be represented as a Kronecker product over dimensions
  \[
    \Kmm = K_{m_1 m_1}^1 \otimes K^2_{m_2 m_2} \otimes \ldots \otimes
    K^D_{m_D m_D},
  \]
  where
  \[
    K^i_{m_i m_i} = K_i(Z_i, Z_i) \in \R^{m_i \times m_i}~~~\forall i = 1, 2, \ldots, D.
  \]
  Kronecker products allow efficient computation of inverse
  \[
    (A_1 \otimes A_2 \otimes \ldots \otimes A_D)^{-1} =
    A_1^{-1} \otimes A_2^{-1} \otimes \ldots \otimes A_D^{-1},
  \]
  and determinant
  \[
    |A_1 \otimes A_2 \otimes \ldots \otimes A_D| = |A_1|^{c_1} \cdot
    |A_2|^{c_2} \cdot \ldots \cdot |A_D|^{c_D},
  \]
  where
  \[
    A_i\in\R^{k_i \times k_i},~~~c_i = \prod_{j \ne i} k_j,~~\forall i = 1, 2, \ldots, D.
  \]

  Another major idea of KISS-GP is to use interpolation to approximate $\Kmn$.
  Considering inducing inputs as interpolation points for the function
  $k(\cdot, z_i)$ we can write
  \begin{equation}
  \label{kernel_interpolation}
    \Kmn \approx \Kmm W,~~~\ki \approx \Kmm w_i,
  \end{equation}
  where $W \in \R^{m \times n}$ contains the coefficients of interpolation, and
  $w_i$ is it's $i$-th column. Authors of KISS-GP suggest using cubic
  convolutional interpolation (\citet{keys1981}), in which case the interpolation
  weights $w_i$ can be represented as a Kronecker product over dimensions
  \[
    w_i = w_i^1 \otimes w_i^2 \otimes \ldots \otimes w_i^D,~~~~~w_i \in \R^{m_i}~~~\forall i = 1, 2, \ldots, D.
  \]
  \citet{wilson2015} combine these ideas with SOR (\citet{silverman1985})
  in the KISS-GP method with $\bigO(n + D m^{1 + 1/D})$ computational
  complexity. This complexity allows to use KISS-GP with a large number (possibly
  greater than $n$) of inducing points. Note, however, that $m$ grows
  exponentially with the dimensionality $D$ of the feature space, and the
  method becomes impractical when $D \gg 4$. We don't describe other details of
  KISS-GP here.

\subsection{Tensor Train Decomposition}
\label{tensor_train}

  Tensor Train (TT) decomposition proposed in \citet{oseledets2011} allow to
  efficiently store tensors (multidimensional arrays of data), large matrices and
  vectors. For matrices and vectors in TT-format linear algebra operations
  can be implemented efficiently. TT format was successfully applied for
  different machine learning tasks (see \citet{novikov2014}, \citet{novikov2015}).

  Consider a $D$-dimensional tensor $\mathcal A \in \R^{k_1 \times k_2 \times \ldots \times k_D}$.
  $\mathcal{A}$ is said to be in the Tensor Train format if
  \begin{equation}
  \label{tt}
    \mathcal{A}(i_1, i_2, \ldots, i_d) = G_1[i_1] \cdot G_2[i_2] \cdot \ldots \cdot G_D[i_D],~~~
    i_k \in \{1, 2, \ldots, n_k\}~~\forall k,
  \end{equation}
  where
  \[
    G_k[i_k] \in \R^{r_k\times r_{k+1}}~~\forall k, i_k,~~~r_0 = r_{D+1} = 1.
  \]
  Matrices $G_k$ are called TT-cores, and numbers $r_k$ are called TT-ranks of
  tensor $\mathcal{A}$.

  In order to represent a vector in TT-format, it is reshaped to a multidimensional
  tensor (possibly with zero padding) and then format (\ref{tt}) is used. We will
  use TT-format for the vector $\mu$ of expectations of the values $u$ of the
  Gaussian process in points $Z$ placed on a multidimensional grid. In this case,
  $\mu$ is naturally represented as a $D$-dimensional tensor.

  For matrices TT format is given by
  \[
    M(i_1, i_2, \ldots, i_d; j_1, j_2, \ldots, j_D) = G_1 [i_1, j_1] \cdot
    G_2[i_2, j_2] \cdot \ldots \cdot G_D[i_D, j_D],
  \]
  where
  \[
    G_k[i_k, j_k] \in \R^{r_k\times r_{k+1}}~~\forall k, i_k, j_k,~~~r_0 = r_{D+1} = 1.
  \]
  Note, that Kronecker product format is a special case of TT for TT-ranks
  $r_1 = r_2 = \ldots = r_{D+1} = 1$.

  Many operations of linear algebra can be efficiently implemented for TT vectors
  and matrices. Let $u, v \in \R^{n_1 \cdot n_2 \cdot \ldots \cdot n_D}$ be vectors
  in TT-format with TT-ranks not greater than $r$.
  \[
    u(i_1, i_2, \ldots, i_D) = u_1[i_1] \cdot u_2[i_2] \cdot \ldots \cdot u_D[i_D],~~~
    i_k \in \{1, 2, \ldots, n_k\}~~\forall k,
  \]
  and the same for $v$.
  Let $A$ and $B$ be represented as a Kronecker product
  \[
    A = A_1 \otimes A_2 \otimes \ldots \otimes A_D,~~~A_k \in \R^{n_k \times n_k}~~\forall k,
  \]
  and the same for $B$. Let $n = \max_k n_k$. Then the computational complexity
  of computing the quadratic form $u^T A v$ is $\bigO(Dnr^3)$. The computational
  complexity of computing $\tr(AB)$ is $\bigO(Dn^2)$. We will need these two
  operations below.
